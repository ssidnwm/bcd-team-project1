{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "import pprint as ppr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 및 전역 변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tiktoken_len(input):\n",
    "    global tokenizer\n",
    "    \n",
    "    if type(input)!=str:\n",
    "        input = str(input)\n",
    "        \n",
    "    tokens = tokenizer.encode(input)\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkGroup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.info = []\n",
    "        \n",
    "    def load(self, dict_loaded: dict, func_tokenLength):\n",
    "        self.base = dict_loaded\n",
    "        self.tklenGet = func_tokenLength\n",
    "        \n",
    "        keys_subDt = [ [key_subDt] for key_subDt in ChunkGroup.keyParser(self.base)[0] ]\n",
    "        self.info = keys_subDt            \n",
    "        \n",
    "    def chunkMerge(self, tkSize_chunk, tkSize_overlap, opt_leftover=\"Retain\"):\n",
    "        info_chunkGroups = []\n",
    "        group = ChunkGroup.__boardReset()\n",
    "        overlap = ChunkGroup.__boardReset()\n",
    "        \n",
    "        keys_subDt_inList = [key_subDt for key_subDt in self.info if type(key_subDt) == list]\n",
    "        keys_subDt_merged = [key_subDt_inList[0] for key_subDt_inList in keys_subDt_inList if len(key_subDt_inList) == 1]\n",
    "        keys_subDt_notMerged = [key_subDt for key_subDt in self.info if key_subDt not in [ [key_subDt_merged] for key_subDt_merged in keys_subDt_merged]]\n",
    "        for key_subDt_merge in keys_subDt_merged:\n",
    "            \n",
    "            if group[\"tklength_Sum\"] == 0 and overlap[\"tklength_Sum\"] != 0:\n",
    "                ChunkGroup.__boardUpdate(group, overlap[\"tklength_Sum\"], overlap[\"keys\"])\n",
    "                overlap = ChunkGroup.__boardReset()\n",
    "            \n",
    "            ChunkGroup.__boardUpdate(group, self.tklenGet(self.base[key_subDt_merge]), [key_subDt_merge])\n",
    "        \n",
    "            if tkSize_chunk > group[\"tklength_Sum\"] > tkSize_chunk-tkSize_overlap:\n",
    "                ChunkGroup.__boardUpdate(overlap, self.tklenGet(self.base[key_subDt_merge]), [key_subDt_merge])\n",
    "                \n",
    "            if group[\"tklength_Sum\"] >= tkSize_chunk:\n",
    "                info_chunkGroups.append(group[\"keys\"])\n",
    "                group = ChunkGroup.__boardReset()\n",
    "                \n",
    "        if opt_leftover==\"Retain\":\n",
    "            info_chunkGroups.append(group[\"keys\"])\n",
    "            \n",
    "        elif opt_leftover==\"Forced\":\n",
    "            group[\"keys\"].reverse() \n",
    "            for key_notIn_Group in reversed([key_subDt for key_subDt in self.base if key_subDt not in group[\"keys\"]]):\n",
    "                ChunkGroup.__boardUpdate(group, self.tklenGet(self.base[key_notIn_Group]), [key_notIn_Group]) \n",
    "                if group[\"tklength_Sum\"] >= tkSize_chunk:\n",
    "                    group[\"keys\"].reverse()  \n",
    "                    info_chunkGroups.append(group[\"keys\"])\n",
    "                    break\n",
    "        \n",
    "        self.info = keys_subDt_notMerged        \n",
    "        self.info.extend(info_chunkGroups)\n",
    "        \n",
    "        \n",
    "    def chunkSplit(self, tkSize_chunk, tkSize_overlap, opt_leftover=\"Retain\"):\n",
    "        keys_subDt_inList = [key_subDt_inList for key_subDt_inList in self.info if type(key_subDt_inList) == list]\n",
    "        keys_subDt = [key_subDt_inList[0] for key_subDt_inList in keys_subDt_inList if len(key_subDt_inList) == 1]\n",
    "        keys_subDt_splited = [key_subDt for key_subDt in keys_subDt if self.tklenGet(self.base[key_subDt]) > tkSize_chunk]\n",
    "        for key_subDt_splited in keys_subDt_splited:\n",
    "            \n",
    "            sub_ChunkGroup = ChunkGroup()\n",
    "            sub_ChunkGroup.load(self.base[key_subDt_splited], self.tklenGet)\n",
    "             \n",
    "            sub_ChunkGroup.chunkMerge(tkSize_chunk, tkSize_overlap, opt_leftover=opt_leftover)\n",
    "                        \n",
    "            self.info[self.info.index([key_subDt_splited])] = {key_subDt_splited:sub_ChunkGroup.info}   \n",
    "    \n",
    "    \n",
    "    def chunkMake(self):\n",
    "        self.chunks = []\n",
    "        keys_common = ChunkGroup.keyParser(self.base)[1]\n",
    "        \n",
    "        for chunkGroup_info in self.info:\n",
    "            \n",
    "            if type(chunkGroup_info) == list:\n",
    "                keys_grouped = chunkGroup_info\n",
    "                keys_grouped = keys_common + keys_grouped\n",
    "                \n",
    "                chunk = {key_grouped: self.base[key_grouped] for key_grouped in keys_grouped}\n",
    "                \n",
    "                self.chunks.append(chunk)\n",
    "                \n",
    "            elif type(chunkGroup_info) == dict:\n",
    "                key_splited = list(chunkGroup_info.keys())[0]\n",
    "                keysL_grouped_inSplited = chunkGroup_info[key_splited]\n",
    "                \n",
    "                chunkGroup_inSplited = ChunkGroup()\n",
    "                chunkGroup_inSplited.load(self.base[key_splited], None)\n",
    "                \n",
    "                chunks_inSplited = []\n",
    "                for keys_grouped_inSplited in keysL_grouped_inSplited:\n",
    "                   chunkGroup_inSplited.info = [keys_grouped_inSplited]\n",
    "                   \n",
    "                   chunk_inSplited = {key_splited:chunkGroup_inSplited.chunkMake()}\n",
    "                   for key_common in keys_common: chunk_inSplited[key_common] = self.base[key_common]\n",
    "                   \n",
    "                   chunks_inSplited.append(chunk_inSplited)\n",
    "                self.chunks.extend(chunks_inSplited)\n",
    "                   \n",
    "        return self.chunks\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def keyParser(dict_parsed):\n",
    "        keys_for_dict = [key_in_dict_parsed for key_in_dict_parsed in list(dict_parsed.keys()) if type(dict_parsed.get(key_in_dict_parsed)) == dict]\n",
    "        keys_for_else = [key_in_dict_parsed for key_in_dict_parsed in list(dict_parsed.keys()) if key_in_dict_parsed not in keys_for_dict] \n",
    "        \n",
    "        return keys_for_dict, keys_for_else\n",
    "    \n",
    "    @staticmethod\n",
    "    def __boardReset():\n",
    "            return {\"tklength_Sum\":0, \"keys\":[]}\n",
    "        \n",
    "    @staticmethod\n",
    "    def __boardUpdate(input_board: dict, input_tkLength, input_key):\n",
    "        input_board[\"tklength_Sum\"] += input_tkLength\n",
    "        input_board[\"keys\"].extend(input_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워킹 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetChapter = 1 \n",
    "\n",
    "data_folder=r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSON_byChapter_v2\\R078r3e_Annex3_Appendix1'\n",
    "data_fileName = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_ver2.json'\n",
    "\n",
    "# 텍스트 추출\n",
    "with open(data_folder+'\\\\'+data_fileName , 'r') as source:\n",
    "    dict_chapter = json.load(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지정한 청킹 및 오버랩 사이즈를 바탕으로 챕터 청킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkSize_chunk_global = 500\n",
    "tkSize_overlap_global = 100\n",
    "tklen_func_global = tiktoken_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkGroup_targetChapter = ChunkGroup()\n",
    "\n",
    "ChunkGroup_targetChapter.load(dict_chapter, tklen_func_global)\n",
    "\n",
    "ChunkGroup_targetChapter.chunkSplit(tkSize_chunk_global, tkSize_overlap_global)\n",
    "ChunkGroup_targetChapter.chunkMerge(tkSize_chunk_global,tkSize_overlap_global)\n",
    "\n",
    "Chunks_targetChapter = ChunkGroup_targetChapter.chunkMake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메타 데이터를 기록하여 JSONL 형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSON_byChapter_v2\\R078r3e_Annex3_Appendix1'\n",
    "file_name = data_fileName = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_ver2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + '\\\\'+ file_name, encoding= \"utf-8\",mode=\"w\") as file: \n",
    "\tfor chunk_targetChapter in Chunks_targetChapter: file.write(json.dumps(chunk_targetChapter) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Chapter\": \"1\", \"Title\": \"Alternative method for the determination of peak braking coefficient (PBC)\", \"1.1.\": {\"Description\": [\"General:\"], \"Item\": [\"(a) The test is to establish a PBC for the vehicle type when being brakedon the test surfaces described in Annex 3, paragraphs 1. 1.1. and 1.1.2.\", \"(b) The test comprises a number of stops with varying brake control forces. Both wheels shall be braked simultaneously up to the point reached before the wheel lock, in order to achieve the maximum vehicle deceleration rate on the given test surface.\", \"(c) The maximum vehicle deceleration rate is the highest value recorded during all the test stops.\", \"(d) The Peak Braking Coefficient (PBC) is calculated from the test stop that generates the maximum vehicle deceleration rate, as follows: [Equation 1]\", \"(e) The value of PBC shall be rounded to two decimal places.\"], \"Equation 1\": [\"_PBC_ =0.566/ _t_\", \"\", \"where:\", \"\", \"t = time taken for the vehicle speed to reduce from 40 km/h to 20 km/h\", \"\", \"in seconds.\", \"\", \"_Note: For vehicles unable to achieve a test speed of 50 km/h, PBC shall be_\", \"\", \"measured as follows:\", \"\", \"_PBC_ =0.566/ _t_\", \"\", \"where:\", \"\", \"t = time taken, in seconds, for the speed of the vehicle to reduce\", \"\", \"from 0.8 Vmax to (0.8 Vmax - 20), where Vmax is measured in km/h.\"]}, \"1.2.\": {\"Description\": [\"Vehicle condition:\"], \"Item\": [\"(a) The test is applicable to all vehicle categories.\", \"(b) The anti-lock system shall be either disconnected or inoperative (ABSfunction disabled), between 40 km/h and 20 km/h.\", \"(c) Lightly loaded.\", \"(d) Engine disconnected.\"]}, \"1.3.\": {\"Description\": [\"Test conditions and procedure:\"], \"Item\": [\"(a) Initial brake temperature: \\u2265 55 \\u00b0C and \\u2264 100 \\u00b0C.\", \"(b) Test speed: 60 km/h or 0.9 Vmax, whichever is lower.\", \"(c) Brake application: Simultaneous actuation of both service brake system controls, if so equipped, or of the single service brake system control in the case of a service brake system that operates on all wheels. For vehicles equipped with a single service brake system control, it may be necessary to modify the brake system if one of the wheels is not approaching maximum deceleration.\", \"(d) Brake actuation force: The control force that achieves the maximum vehicle deceleration rate as defined in paragraph 1.1.(c). The application of the control force shall be constant during braking.\", \"(e) Number of stops: until the vehicle meets its maximum deceleration rate.\", \"(f) For each stop, accelerate the vehicle to the test speed and then actuate the brake control(s) under the conditions specified in this paragraph.\"]}}\n",
      "\n",
      "{\"Chapter\": \"1\", \"Title\": \"Alternative method for the determination of peak braking coefficient (PBC)\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_path + '\\\\'+ file_name) as f: \n",
    "\tfor line in f: print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워킹 코드 (자동화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 챕터 별 개별 저장\n",
    "for targetChapter in range(1,2): \n",
    "\n",
    "    # 데이터 입출력 경로 지정\n",
    "    \n",
    "    data_folder=r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSON_byChapter_v2\\R078r3e_Annex3_Appendix1'\n",
    "    data_fileName = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_ver2.json'\n",
    "\n",
    "    data_folder_out = r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSONL_byChunk_v2\\R078r3e_Annex3_Appendix1'\n",
    "    data_fileName_out = data_fileName = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_chunked_ver2.jsonl'\n",
    "\n",
    "    # 청크 사이즈 & 토크나이저 지정\n",
    "    tkSize_chunk_global = 500\n",
    "    tkSize_overlap_global = 100\n",
    "    tklen_func_global = tiktoken_len\n",
    "\n",
    "    # 원본 JSON 불러오기\n",
    "    with open(data_folder_in+'\\\\'+data_fileName_in , 'r') as source:\n",
    "        dict_chapter = json.load(source)\n",
    "        \n",
    "    # 청킹    \n",
    "    ChunkGroup_targetChapter = ChunkGroup()\n",
    "\n",
    "    ChunkGroup_targetChapter.load(dict_chapter, tklen_func_global)\n",
    "\n",
    "    ChunkGroup_targetChapter.chunkSplit(tkSize_chunk_global, tkSize_overlap_global)\n",
    "    ChunkGroup_targetChapter.chunkMerge(tkSize_chunk_global,tkSize_overlap_global)\n",
    "\n",
    "    Chunks_targetChapter = ChunkGroup_targetChapter.chunkMake()\n",
    "\n",
    "    # 청킹 가공된 JSON 저장하기\n",
    "    with open(data_folder_out + '\\\\'+ data_fileName_out, encoding= \"utf-8\",mode=\"w\") as file: \n",
    "        for chunk_targetChapter in Chunks_targetChapter: file.write(json.dumps(chunk_targetChapter) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 챕터 통합 저장\n",
    "Chunks_fullChapter = []\n",
    "\n",
    "for targetChapter in range(1,2): \n",
    "\n",
    "    # 데이터 입출력 경로 지정\n",
    "    data_folder_in = r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSON_byChapter_v2\\R078r3e_Annex3_Appendix1'\n",
    "    data_fileName_in = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_ver2.json'\n",
    "\n",
    "    # 청크 사이즈 & 토크나이저 지정\n",
    "    tkSize_chunk_global = 500\n",
    "    tkSize_overlap_global = 100\n",
    "    tklen_func_global = tiktoken_len\n",
    "\n",
    "    # 원본 JSON 불러오기\n",
    "    with open(data_folder_in+'\\\\'+data_fileName_in , 'r') as source:\n",
    "        dict_chapter = json.load(source)\n",
    "        \n",
    "    # 청킹    \n",
    "    ChunkGroup_targetChapter = ChunkGroup()\n",
    "\n",
    "    ChunkGroup_targetChapter.load(dict_chapter, tklen_func_global)\n",
    "\n",
    "    ChunkGroup_targetChapter.chunkSplit(tkSize_chunk_global, tkSize_overlap_global)\n",
    "    ChunkGroup_targetChapter.chunkMerge(tkSize_chunk_global,tkSize_overlap_global)\n",
    "\n",
    "    Chunks_targetChapter = ChunkGroup_targetChapter.chunkMake()\n",
    "\n",
    "    Chunks_fullChapter.extend(Chunks_targetChapter)\n",
    "    \n",
    "# 청킹 가공된 JSON 저장하기\n",
    "    data_folder_out = r'G:\\내 드라이브\\LAB_works\\법률 LLM 프로젝트\\data\\데이터 전처리\\3. JSON 컨버팅\\JSONL_byChunk_v2\\R078r3e_Annex3_Appendix1'\n",
    "    data_fileName_out = data_fileName = fr'R078r3e_Annex3_Appendix1_chapter{targetChapter}_chunked_ver2.jsonl'\n",
    "\n",
    "\n",
    "with open(data_folder_out + '\\\\'+ data_fileName_out, encoding= \"utf-8\",mode=\"w\") as file: \n",
    "    for Chunk_fullChapter in Chunks_fullChapter: file.write(json.dumps(Chunk_fullChapter) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
